nohup: ignoring input
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 15:21:17,464 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=500, select_method='random')
[2022-08-17 15:21:18,095 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 15:21:19,005 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 15:21:19,517 INFO] (snngp_inference:59) inducing_points shape: (500, 784)
[2022-08-17 15:21:19,528 INFO] (snngp:271) Optimizing...
2022-08-17 15:21:25.870251: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:21:26.734948: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1.864849367s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:21:29.037809: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:22:40.586500: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1m13.54875565s
Constant folding an instruction is taking > 2s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 15:23:07,544 INFO] (snngp:279) Optimized for 13 iters; Success: True; Result: [0.89772905 0.07221421], 0.010000000000000016
[2022-08-17 15:23:19,275 INFO] (isnngp_inference:99) LML: 3445.0617
[2022-08-17 15:23:23,384 INFO] (isnngp_inference:101) ELBO: -940.6758
[2022-08-17 15:23:23,748 INFO] (isnngp_inference:103) EUBO: 13474.2446
[2022-08-17 15:23:26,039 INFO] (isnngp_inference:107) Accuracy: 92.66%
[2022-08-17 15:23:26,125 INFO] (isnngp_inference:109) Loss: 0.1928
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 15:23:41,151 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=1000, select_method='random')
[2022-08-17 15:23:41,668 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 15:23:41,864 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 15:23:42,336 INFO] (snngp_inference:59) inducing_points shape: (1000, 784)
[2022-08-17 15:23:42,348 INFO] (snngp:271) Optimizing...
2022-08-17 15:23:48.699973: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:23:54.936751: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 7.236850251s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:23:57.269738: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:26:20.748591: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2m25.478913379s
Constant folding an instruction is taking > 2s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 15:27:12,208 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [0.94407203 0.10862676], 0.010000000000000016
[2022-08-17 15:27:24,657 INFO] (isnngp_inference:99) LML: 4165.3561
[2022-08-17 15:27:29,490 INFO] (isnngp_inference:101) ELBO: 1145.5125
[2022-08-17 15:27:30,139 INFO] (isnngp_inference:103) EUBO: 13286.8824
[2022-08-17 15:27:32,894 INFO] (isnngp_inference:107) Accuracy: 94.59%
[2022-08-17 15:27:32,976 INFO] (isnngp_inference:109) Loss: 0.1857
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 15:27:46,973 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=1500, select_method='random')
[2022-08-17 15:27:47,447 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 15:27:47,670 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 15:27:48,135 INFO] (snngp_inference:59) inducing_points shape: (1500, 784)
[2022-08-17 15:27:48,148 INFO] (snngp:271) Optimizing...
2022-08-17 15:27:54.560345: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:28:09.989211: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 16.428938957s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:28:12.320567: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:31:52.273901: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 3m41.95340041s
Constant folding an instruction is taking > 2s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 15:33:28,810 INFO] (snngp:279) Optimized for 12 iters; Success: True; Result: [0.96856354 0.1261636 ], 0.010000000000000016
[2022-08-17 15:33:40,697 INFO] (isnngp_inference:99) LML: 4496.9681
[2022-08-17 15:33:46,602 INFO] (isnngp_inference:101) ELBO: 2053.4175
[2022-08-17 15:33:47,667 INFO] (isnngp_inference:103) EUBO: 13146.7593
[2022-08-17 15:33:51,235 INFO] (isnngp_inference:107) Accuracy: 95.15%
[2022-08-17 15:33:51,328 INFO] (isnngp_inference:109) Loss: 0.1838
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 15:34:05,213 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=2000, select_method='random')
[2022-08-17 15:34:05,789 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 15:34:05,998 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 15:34:06,462 INFO] (snngp_inference:59) inducing_points shape: (2000, 784)
[2022-08-17 15:34:06,474 INFO] (snngp:271) Optimizing...
2022-08-17 15:34:13.041168: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:34:41.019648: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 28.978551871s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:34:43.378114: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:39:31.989195: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 4m50.611150225s
Constant folding an instruction is taking > 2s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 15:41:41,868 INFO] (snngp:279) Optimized for 10 iters; Success: True; Result: [0.98482864 0.13313334], 0.010000000000000016
[2022-08-17 15:41:53,972 INFO] (isnngp_inference:99) LML: 4709.7319
[2022-08-17 15:42:01,141 INFO] (isnngp_inference:101) ELBO: 2631.0902
[2022-08-17 15:42:02,670 INFO] (isnngp_inference:103) EUBO: 13040.6417
[2022-08-17 15:42:07,084 INFO] (isnngp_inference:107) Accuracy: 95.53%
[2022-08-17 15:42:07,203 INFO] (isnngp_inference:109) Loss: 0.1820
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 15:42:21,198 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=2500, select_method='random')
[2022-08-17 15:42:21,760 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 15:42:21,966 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 15:42:22,455 INFO] (snngp_inference:59) inducing_points shape: (2500, 784)
[2022-08-17 15:42:22,467 INFO] (snngp:271) Optimizing...
2022-08-17 15:42:29.024525: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:43:13.564770: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 45.540318279s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 15:46:43,986 INFO] (snngp:279) Optimized for 12 iters; Success: True; Result: [0.99752836 0.13744649], 0.010000000000000016
[2022-08-17 15:46:55,804 INFO] (isnngp_inference:99) LML: 4869.5433
[2022-08-17 15:47:04,196 INFO] (isnngp_inference:101) ELBO: 3002.5126
[2022-08-17 15:47:06,394 INFO] (isnngp_inference:103) EUBO: 12946.4395
[2022-08-17 15:47:11,445 INFO] (isnngp_inference:107) Accuracy: 95.85%
[2022-08-17 15:47:11,526 INFO] (isnngp_inference:109) Loss: 0.1813
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 15:47:25,483 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=3000, select_method='random')
[2022-08-17 15:47:25,960 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 15:47:26,149 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 15:47:26,613 INFO] (snngp_inference:59) inducing_points shape: (3000, 784)
[2022-08-17 15:47:26,625 INFO] (snngp:271) Optimizing...
2022-08-17 15:47:33.237625: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:48:37.622367: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1m5.384813047s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 15:53:06,169 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.00881333 0.13961549], 0.010000000000000016
[2022-08-17 15:53:18,378 INFO] (isnngp_inference:99) LML: 5008.6538
[2022-08-17 15:53:28,574 INFO] (isnngp_inference:101) ELBO: 3305.5997
[2022-08-17 15:53:31,667 INFO] (isnngp_inference:103) EUBO: 12859.3389
[2022-08-17 15:53:37,416 INFO] (isnngp_inference:107) Accuracy: 95.97%
[2022-08-17 15:53:37,496 INFO] (isnngp_inference:109) Loss: 0.1805
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 15:53:51,463 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=3500, select_method='random')
[2022-08-17 15:53:51,943 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 15:53:52,134 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 15:53:52,600 INFO] (snngp_inference:59) inducing_points shape: (3500, 784)
[2022-08-17 15:53:52,614 INFO] (snngp:271) Optimizing...
2022-08-17 15:53:59.236874: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 15:55:27.462103: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1m29.225303538s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 16:00:49,369 INFO] (snngp:279) Optimized for 10 iters; Success: True; Result: [1.0166422  0.13987688], 0.010000000000000016
[2022-08-17 16:01:01,477 INFO] (isnngp_inference:99) LML: 5104.1065
[2022-08-17 16:01:13,891 INFO] (isnngp_inference:101) ELBO: 3518.8744
[2022-08-17 16:01:17,838 INFO] (isnngp_inference:103) EUBO: 12788.3379
[2022-08-17 16:01:24,617 INFO] (isnngp_inference:107) Accuracy: 96.03%
[2022-08-17 16:01:24,697 INFO] (isnngp_inference:109) Loss: 0.1801
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 16:01:38,533 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=4000, select_method='random')
[2022-08-17 16:01:39,018 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 16:01:39,263 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 16:01:39,786 INFO] (snngp_inference:59) inducing_points shape: (4000, 784)
[2022-08-17 16:01:39,799 INFO] (snngp:271) Optimizing...
2022-08-17 16:01:46.493319: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 16:03:49.433904: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2m3.940664034s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 16:11:09,088 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.02614152 0.14198332], 0.010000000000000016
[2022-08-17 16:11:21,020 INFO] (isnngp_inference:99) LML: 5212.8560
[2022-08-17 16:11:35,659 INFO] (isnngp_inference:101) ELBO: 3745.7545
[2022-08-17 16:11:40,961 INFO] (isnngp_inference:103) EUBO: 12710.3936
[2022-08-17 16:11:48,511 INFO] (isnngp_inference:107) Accuracy: 96.08%
[2022-08-17 16:11:48,591 INFO] (isnngp_inference:109) Loss: 0.1795
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 16:12:02,511 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=4500, select_method='random')
[2022-08-17 16:12:02,992 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 16:12:03,182 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 16:12:03,655 INFO] (snngp_inference:59) inducing_points shape: (4500, 784)
[2022-08-17 16:12:03,669 INFO] (snngp:271) Optimizing...
2022-08-17 16:12:10.455507: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 16:14:36.682878: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2m27.234570014s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 16:23:50,921 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.03163572 0.14007931], 0.010000000000000016
[2022-08-17 16:24:04,651 INFO] (isnngp_inference:99) LML: 5279.5151
[2022-08-17 16:24:21,275 INFO] (isnngp_inference:101) ELBO: 3895.1624
[2022-08-17 16:24:27,696 INFO] (isnngp_inference:103) EUBO: 12650.5861
[2022-08-17 16:24:36,237 INFO] (isnngp_inference:107) Accuracy: 96.22%
[2022-08-17 16:24:36,317 INFO] (isnngp_inference:109) Loss: 0.1794
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 16:24:50,169 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=5000, select_method='random')
[2022-08-17 16:24:50,657 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 16:24:50,850 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 16:24:51,334 INFO] (snngp_inference:59) inducing_points shape: (5000, 784)
[2022-08-17 16:24:51,348 INFO] (snngp:271) Optimizing...
2022-08-17 16:24:58.228310: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 16:27:59.151871: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 3m1.923643328s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 16:38:30,090 INFO] (snngp:279) Optimized for 10 iters; Success: True; Result: [1.03932336 0.1399071 ], 0.010000000000000016
[2022-08-17 16:38:42,553 INFO] (isnngp_inference:99) LML: 5365.9330
[2022-08-17 16:39:02,302 INFO] (isnngp_inference:101) ELBO: 4057.4488
[2022-08-17 16:39:10,561 INFO] (isnngp_inference:103) EUBO: 12579.8496
[2022-08-17 16:39:20,100 INFO] (isnngp_inference:107) Accuracy: 96.32%
[2022-08-17 16:39:20,181 INFO] (isnngp_inference:109) Loss: 0.1789
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 16:39:34,177 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=5500, select_method='random')
[2022-08-17 16:39:34,649 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 16:39:34,835 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 16:39:35,314 INFO] (snngp_inference:59) inducing_points shape: (5500, 784)
[2022-08-17 16:39:35,329 INFO] (snngp:271) Optimizing...
2022-08-17 16:39:42.381199: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 16:43:21.397774: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 3m40.016647786s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 16:56:59,508 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.04399964 0.14091008], 0.010000000000000016
[2022-08-17 16:57:11,464 INFO] (isnngp_inference:99) LML: 5415.3418
[2022-08-17 16:57:34,121 INFO] (isnngp_inference:101) ELBO: 4166.9245
[2022-08-17 16:57:43,391 INFO] (isnngp_inference:103) EUBO: 12531.7544
[2022-08-17 16:57:54,753 INFO] (isnngp_inference:107) Accuracy: 96.39%
[2022-08-17 16:57:54,833 INFO] (isnngp_inference:109) Loss: 0.1787
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 16:58:08,973 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=6000, select_method='random')
[2022-08-17 16:58:09,454 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 16:58:09,657 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 16:58:10,149 INFO] (snngp_inference:59) inducing_points shape: (6000, 784)
[2022-08-17 16:58:10,165 INFO] (snngp:271) Optimizing...
2022-08-17 16:58:17.210954: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 17:02:39.531833: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 4m23.320952218s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 17:02:42.613214: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  multiply.460 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 17:02:42.915782: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2.302641316s
Constant folding an instruction is taking > 2s:

  multiply.460 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 17:19:14,799 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.05025572 0.13978152], 0.010000000000000016
[2022-08-17 17:19:26,794 INFO] (isnngp_inference:99) LML: 5484.0173
[2022-08-17 17:19:52,389 INFO] (isnngp_inference:101) ELBO: 4285.8519
[2022-08-17 17:20:03,655 INFO] (isnngp_inference:103) EUBO: 12466.1786
[2022-08-17 17:20:15,974 INFO] (isnngp_inference:107) Accuracy: 96.31%
[2022-08-17 17:20:16,056 INFO] (isnngp_inference:109) Loss: 0.1784
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 17:20:30,078 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=6500, select_method='random')
[2022-08-17 17:20:30,559 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 17:20:30,768 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 17:20:31,241 INFO] (snngp_inference:59) inducing_points shape: (6500, 784)
[2022-08-17 17:20:31,253 INFO] (snngp:271) Optimizing...
2022-08-17 17:20:38.460885: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 17:25:44.996580: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 5m7.535773344s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 17:25:48.062894: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  multiply.460 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-17 17:25:48.756802: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2.694018598s
Constant folding an instruction is taking > 2s:

  multiply.460 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-17 17:44:53,166 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.05514624 0.13844905], 0.010000000000000016
[2022-08-17 17:45:05,080 INFO] (isnngp_inference:99) LML: 5537.1063
[2022-08-17 17:45:34,865 INFO] (isnngp_inference:101) ELBO: 4402.1305
[2022-08-17 17:45:48,571 INFO] (isnngp_inference:103) EUBO: 12410.9589
[2022-08-17 17:46:01,657 INFO] (isnngp_inference:107) Accuracy: 96.43%
[2022-08-17 17:46:01,736 INFO] (isnngp_inference:109) Loss: 0.1782
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 17:46:15,920 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=7000, select_method='random')
[2022-08-17 17:46:16,404 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 17:46:16,593 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 17:46:17,095 INFO] (snngp_inference:59) inducing_points shape: (7000, 784)
[2022-08-17 17:46:17,109 INFO] (snngp:271) Optimizing...
[2022-08-17 18:09:15,354 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.06038584 0.13786041], 0.010000000000000016
[2022-08-17 18:09:27,144 INFO] (isnngp_inference:99) LML: 5591.4310
[2022-08-17 18:10:00,280 INFO] (isnngp_inference:101) ELBO: 4509.3869
[2022-08-17 18:10:15,705 INFO] (isnngp_inference:103) EUBO: 12351.7134
[2022-08-17 18:10:31,549 INFO] (isnngp_inference:107) Accuracy: 96.43%
[2022-08-17 18:10:31,630 INFO] (isnngp_inference:109) Loss: 0.1779
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 18:10:45,571 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=7500, select_method='random')
[2022-08-17 18:10:46,052 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 18:10:46,268 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 18:10:46,840 INFO] (snngp_inference:59) inducing_points shape: (7500, 784)
[2022-08-17 18:10:46,857 INFO] (snngp:271) Optimizing...
[2022-08-17 18:36:45,998 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.06473376 0.13788319], 0.010000000000000016
[2022-08-17 18:36:57,980 INFO] (isnngp_inference:99) LML: 5634.8183
[2022-08-17 18:37:34,970 INFO] (isnngp_inference:101) ELBO: 4594.1699
[2022-08-17 18:37:52,223 INFO] (isnngp_inference:103) EUBO: 12302.8078
[2022-08-17 18:38:08,533 INFO] (isnngp_inference:107) Accuracy: 96.48%
[2022-08-17 18:38:08,619 INFO] (isnngp_inference:109) Loss: 0.1777
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 18:38:22,526 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=8000, select_method='random')
[2022-08-17 18:38:22,998 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 18:38:23,192 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 18:38:23,673 INFO] (snngp_inference:59) inducing_points shape: (8000, 784)
[2022-08-17 18:38:23,687 INFO] (snngp:271) Optimizing...
[2022-08-17 19:08:22,395 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.06884309 0.13643681], 0.010000000000000016
[2022-08-17 19:08:34,678 INFO] (isnngp_inference:99) LML: 5677.1736
[2022-08-17 19:09:15,678 INFO] (isnngp_inference:101) ELBO: 4673.1805
[2022-08-17 19:09:35,307 INFO] (isnngp_inference:103) EUBO: 12249.3172
[2022-08-17 19:09:53,556 INFO] (isnngp_inference:107) Accuracy: 96.58%
[2022-08-17 19:09:53,637 INFO] (isnngp_inference:109) Loss: 0.1776
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 19:10:07,605 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=8500, select_method='random')
[2022-08-17 19:10:08,231 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 19:10:08,474 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 19:10:08,958 INFO] (snngp_inference:59) inducing_points shape: (8500, 784)
[2022-08-17 19:10:08,973 INFO] (snngp:271) Optimizing...
[2022-08-17 19:44:26,653 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.07159859 0.13658927], 0.010000000000000016
[2022-08-17 19:44:38,989 INFO] (isnngp_inference:99) LML: 5703.5434
[2022-08-17 19:45:24,339 INFO] (isnngp_inference:101) ELBO: 4722.4498
[2022-08-17 19:45:47,014 INFO] (isnngp_inference:103) EUBO: 12213.4133
[2022-08-17 19:46:06,775 INFO] (isnngp_inference:107) Accuracy: 96.65%
[2022-08-17 19:46:06,855 INFO] (isnngp_inference:109) Loss: 0.1773
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 19:46:21,024 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=9000, select_method='random')
[2022-08-17 19:46:21,509 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 19:46:21,718 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 19:46:22,207 INFO] (snngp_inference:59) inducing_points shape: (9000, 784)
[2022-08-17 19:46:22,222 INFO] (snngp:271) Optimizing...
[2022-08-17 20:25:25,617 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.07667347 0.13559383], 0.010000000000000016
[2022-08-17 20:25:37,732 INFO] (isnngp_inference:99) LML: 5753.0131
[2022-08-17 20:26:28,161 INFO] (isnngp_inference:101) ELBO: 4824.7715
[2022-08-17 20:26:53,246 INFO] (isnngp_inference:103) EUBO: 12153.6343
[2022-08-17 20:27:14,682 INFO] (isnngp_inference:107) Accuracy: 96.59%
[2022-08-17 20:27:14,763 INFO] (isnngp_inference:109) Loss: 0.1772
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 20:27:28,809 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=9500, select_method='random')
[2022-08-17 20:27:29,292 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 20:27:29,486 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 20:27:30,001 INFO] (snngp_inference:59) inducing_points shape: (9500, 784)
[2022-08-17 20:27:30,014 INFO] (snngp:271) Optimizing...
[2022-08-17 21:11:11,173 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.0810817  0.13511994], 0.010000000000000016
[2022-08-17 21:11:23,320 INFO] (isnngp_inference:99) LML: 5794.5217
[2022-08-17 21:12:17,700 INFO] (isnngp_inference:101) ELBO: 4904.1813
[2022-08-17 21:12:46,022 INFO] (isnngp_inference:103) EUBO: 12099.6265
[2022-08-17 21:13:09,293 INFO] (isnngp_inference:107) Accuracy: 96.60%
[2022-08-17 21:13:09,374 INFO] (isnngp_inference:109) Loss: 0.1770
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-17 21:13:23,199 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist20k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist20k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=10000, select_method='random')
[2022-08-17 21:13:23,679 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-17 21:13:23,869 INFO] (data_loader:193) MNIST: (20000, 784) augmented train
[2022-08-17 21:13:24,339 INFO] (snngp_inference:59) inducing_points shape: (10000, 784)
[2022-08-17 21:13:24,354 INFO] (snngp:271) Optimizing...
[2022-08-17 22:03:02,424 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [1.08427053 0.13470556], 0.010000000000000016
[2022-08-17 22:03:14,619 INFO] (isnngp_inference:99) LML: 5824.1125
[2022-08-17 22:04:15,584 INFO] (isnngp_inference:101) ELBO: 4955.9721
[2022-08-17 22:04:47,666 INFO] (isnngp_inference:103) EUBO: 12054.1110
[2022-08-17 22:05:12,494 INFO] (isnngp_inference:107) Accuracy: 96.68%
[2022-08-17 22:05:12,574 INFO] (isnngp_inference:109) Loss: 0.1769
