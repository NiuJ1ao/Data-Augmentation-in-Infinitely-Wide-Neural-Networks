nohup: ignoring input
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 01:55:46,342 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=1000, select_method='random')
[2022-08-23 01:55:47,710 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 01:55:48,369 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 01:55:50,230 INFO] (snngp_inference:59) inducing_points shape: (1000, 784)
[2022-08-23 01:55:50,268 INFO] (snngp:271) Optimizing...
2022-08-23 01:56:33.354483: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instruction is taking > 1s:

  %dot.2 = f64[1000,1000]{1,0} dot(f64[1000,784]{1,0} %constant.35, f64[1000,784]{1,0} %constant.35), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="jit(_training_loss)/jit(main)/jit(_f)/jit(jit__f)/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/vol/bitbucket/yn621/envs/individual-project/lib/python3.8/site-packages/neural_tangents/_src/stax/requirements.py" source_line=385}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-23 01:56:39.237734: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instruction is taking > 2s:

  %reduce-window.1 = f64[1875,1]{1,0} reduce-window(f64[60000,10]{1,0} %constant.24, f64[] %constant.4), window={size=32x10 stride=32x10}, to_apply=%region_5.1496

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-23 02:04:45,120 INFO] (snngp:279) Optimized for 13 iters; Success: True; Result: [0.85190801 0.08915866], 0.010000000000000016
[2022-08-23 02:05:07,618 INFO] (isnngp_inference:93) ELBO: 15587.4632
[2022-08-23 02:05:13,002 INFO] (isnngp_inference:95) EUBO: 82108.8978
[2022-08-23 02:05:19,641 INFO] (isnngp_inference:99) Accuracy: 95.79%
[2022-08-23 02:05:19,917 INFO] (isnngp_inference:101) Loss: 0.1818
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 02:05:29,669 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=3000, select_method='random')
[2022-08-23 02:05:32,558 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 02:05:33,476 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 02:05:35,208 INFO] (snngp_inference:59) inducing_points shape: (3000, 784)
[2022-08-23 02:05:35,238 INFO] (snngp:271) Optimizing...
2022-08-23 02:06:18.239296: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instruction is taking > 1s:

  %dot.2 = f64[3000,3000]{1,0} dot(f64[3000,784]{1,0} %constant.35, f64[3000,784]{1,0} %constant.35), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="jit(_training_loss)/jit(main)/jit(_f)/jit(jit__f)/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/vol/bitbucket/yn621/envs/individual-project/lib/python3.8/site-packages/neural_tangents/_src/stax/requirements.py" source_line=385}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-23 02:06:47.797420: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instruction is taking > 2s:

  %reduce-window.1 = f64[1875,1]{1,0} reduce-window(f64[60000,10]{1,0} %constant.24, f64[] %constant.4), window={size=32x10 stride=32x10}, to_apply=%region_5.1496

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-23 02:34:38,177 INFO] (snngp:279) Optimized for 11 iters; Success: True; Result: [0.91999442 0.11039717], 0.010000000000000016
[2022-08-23 02:35:33,673 INFO] (isnngp_inference:93) ELBO: 28795.8516
[2022-08-23 02:35:49,710 INFO] (isnngp_inference:95) EUBO: 81304.2024
[2022-08-23 02:35:59,574 INFO] (isnngp_inference:99) Accuracy: 97.33%
[2022-08-23 02:35:59,827 INFO] (isnngp_inference:101) Loss: 0.1758
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 02:36:09,573 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=5000, select_method='random')
[2022-08-23 02:36:12,727 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 02:36:14,590 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 02:36:16,458 INFO] (snngp_inference:59) inducing_points shape: (5000, 784)
[2022-08-23 02:36:16,487 INFO] (snngp:271) Optimizing...
2022-08-23 02:37:00.438526: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instruction is taking > 1s:

  %dot.2 = f64[5000,5000]{1,0} dot(f64[5000,784]{1,0} %constant.35, f64[5000,784]{1,0} %constant.35), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="jit(_training_loss)/jit(main)/jit(_f)/jit(jit__f)/dot_general[dimension_numbers=(((1,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/vol/bitbucket/yn621/envs/individual-project/lib/python3.8/site-packages/neural_tangents/_src/stax/requirements.py" source_line=385}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-23 02:38:38.475569: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instruction is taking > 2s:

  %reduce-window.1 = f64[1875,1]{1,0} reduce-window(f64[60000,10]{1,0} %constant.24, f64[] %constant.4), window={size=32x10 stride=32x10}, to_apply=%region_5.1496

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-23 02:38:49.207758: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instruction is taking > 4s:

  %multiply.460 = f64[5000,5000]{1,0} multiply(f64[5000,5000]{1,0} %constant.284, f64[5000,5000]{1,0} %broadcast.434), metadata={op_name="jit(_training_loss)/jit(main)/jit(_f)/jit(jit__f)/div" source_file="/vol/bitbucket/yn621/envs/individual-project/lib/python3.8/site-packages/neural_tangents/_src/stax/requirements.py" source_line=459}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-23 03:36:07,397 INFO] (snngp:279) Optimized for 10 iters; Success: True; Result: [0.95115063 0.11664548], 0.010000000000000016
[2022-08-23 03:37:27,427 INFO] (isnngp_inference:93) ELBO: 32976.0339
[2022-08-23 03:38:04,250 INFO] (isnngp_inference:95) EUBO: 80780.0086
[2022-08-23 03:38:17,346 INFO] (isnngp_inference:99) Accuracy: 97.56%
[2022-08-23 03:38:17,617 INFO] (isnngp_inference:101) Loss: 0.1737
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 03:38:29,027 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=7000, select_method='random')
[2022-08-23 03:38:32,561 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 03:38:35,237 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 03:38:36,865 INFO] (snngp_inference:59) inducing_points shape: (7000, 784)
[2022-08-23 03:38:36,894 INFO] (snngp:271) Optimizing...
2022-08-23 03:39:24.697925: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instruction is taking > 1s:

  %reduce-window.1 = f64[1875,1]{1,0} reduce-window(f64[60000,10]{1,0} %constant.24, f64[] %constant.4), window={size=32x10 stride=32x10}, to_apply=%region_5.1496

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-23 05:12:05,635 INFO] (snngp:279) Optimized for 9 iters; Success: True; Result: [0.9708526  0.12160651], 0.010000000000000016
[2022-08-23 05:14:19,432 INFO] (isnngp_inference:93) ELBO: 35353.5882
[2022-08-23 05:15:21,544 INFO] (isnngp_inference:95) EUBO: 80379.9412
[2022-08-23 05:15:41,686 INFO] (isnngp_inference:99) Accuracy: 97.79%
[2022-08-23 05:15:41,966 INFO] (isnngp_inference:101) Loss: 0.1726
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 20:48:06,801 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=8000, select_method='random')
[2022-08-23 20:48:10,246 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 20:48:13,261 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 20:48:14,927 INFO] (snngp_inference:59) inducing_points shape: (8000, 784)
[2022-08-23 20:51:15,215 INFO] (isnngp_inference:93) ELBO: 36201.1011
[2022-08-23 20:52:42,843 INFO] (isnngp_inference:95) EUBO: 80269.7788
[2022-08-23 20:53:08,102 INFO] (isnngp_inference:99) Accuracy: 97.92%
[2022-08-23 20:53:08,369 INFO] (isnngp_inference:101) Loss: 0.1723
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 20:53:19,230 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=9000, select_method='random')
[2022-08-23 20:53:21,969 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 20:53:25,539 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 20:53:27,488 INFO] (snngp_inference:59) inducing_points shape: (9000, 784)
[2022-08-23 20:57:15,499 INFO] (isnngp_inference:93) ELBO: 37019.1662
[2022-08-23 20:59:37,490 INFO] (isnngp_inference:95) EUBO: 80158.9736
[2022-08-23 21:00:04,434 INFO] (isnngp_inference:99) Accuracy: 97.78%
[2022-08-23 21:00:04,694 INFO] (isnngp_inference:101) Loss: 0.1720
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 21:00:14,773 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=10000, select_method='random')
[2022-08-23 21:00:17,486 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 21:00:21,051 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 21:00:22,858 INFO] (snngp_inference:59) inducing_points shape: (10000, 784)
[2022-08-23 21:04:53,811 INFO] (isnngp_inference:93) ELBO: 37619.2658
[2022-08-23 21:07:14,596 INFO] (isnngp_inference:95) EUBO: 80065.7966
[2022-08-23 21:07:44,508 INFO] (isnngp_inference:99) Accuracy: 97.97%
[2022-08-23 21:07:44,727 INFO] (isnngp_inference:101) Loss: 0.1718
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 21:07:54,550 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=11000, select_method='random')
[2022-08-23 21:07:57,146 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 21:07:59,849 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 21:08:01,718 INFO] (snngp_inference:59) inducing_points shape: (11000, 784)
[2022-08-23 21:13:31,802 INFO] (isnngp_inference:93) ELBO: 38178.3987
[2022-08-23 21:16:26,353 INFO] (isnngp_inference:95) EUBO: 79978.4573
[2022-08-23 21:17:05,144 INFO] (isnngp_inference:99) Accuracy: 97.95%
[2022-08-23 21:17:05,407 INFO] (isnngp_inference:101) Loss: 0.1716
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 21:17:15,997 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=12000, select_method='random')
[2022-08-23 21:17:18,904 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 21:17:22,234 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 21:17:24,051 INFO] (snngp_inference:59) inducing_points shape: (12000, 784)
[2022-08-23 21:23:13,996 INFO] (isnngp_inference:93) ELBO: 38579.2952
[2022-08-23 21:25:43,342 INFO] (isnngp_inference:95) EUBO: 79906.3478
[2022-08-23 21:26:19,865 INFO] (isnngp_inference:99) Accuracy: 98.00%
[2022-08-23 21:26:20,116 INFO] (isnngp_inference:101) Loss: 0.1714
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 21:26:30,592 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=13000, select_method='random')
[2022-08-23 21:26:33,575 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 21:26:36,694 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 21:26:38,366 INFO] (snngp_inference:59) inducing_points shape: (13000, 784)
[2022-08-23 21:32:59,036 INFO] (isnngp_inference:93) ELBO: 38959.7302
[2022-08-23 21:36:24,804 INFO] (isnngp_inference:95) EUBO: 79835.0180
[2022-08-23 21:37:09,744 INFO] (isnngp_inference:99) Accuracy: 98.01%
[2022-08-23 21:37:10,006 INFO] (isnngp_inference:101) Loss: 0.1712
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 21:37:20,993 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=14000, select_method='random')
[2022-08-23 21:37:24,993 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 21:37:27,426 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 21:37:29,290 INFO] (snngp_inference:59) inducing_points shape: (14000, 784)
[2022-08-23 21:44:32,505 INFO] (isnngp_inference:93) ELBO: 39351.0041
[2022-08-23 21:48:32,651 INFO] (isnngp_inference:95) EUBO: 79767.4725
[2022-08-23 21:49:22,710 INFO] (isnngp_inference:99) Accuracy: 98.10%
[2022-08-23 21:49:22,970 INFO] (isnngp_inference:101) Loss: 0.1712
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 21:49:34,012 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=15000, select_method='random')
[2022-08-23 21:49:37,633 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 21:49:40,300 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 21:49:42,155 INFO] (snngp_inference:59) inducing_points shape: (15000, 784)
[2022-08-23 22:01:00,955 INFO] (isnngp_inference:93) ELBO: 39678.5169
[2022-08-23 22:09:24,484 INFO] (isnngp_inference:95) EUBO: 79705.0767
[2022-08-23 22:10:36,543 INFO] (isnngp_inference:99) Accuracy: 98.03%
[2022-08-23 22:10:36,814 INFO] (isnngp_inference:101) Loss: 0.1711
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 22:10:48,917 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=16000, select_method='random')
[2022-08-23 22:11:13,350 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 22:11:16,499 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 22:11:18,890 INFO] (snngp_inference:59) inducing_points shape: (16000, 784)
[2022-08-23 22:24:32,854 INFO] (isnngp_inference:93) ELBO: 39966.4350
[2022-08-23 22:33:39,537 INFO] (isnngp_inference:95) EUBO: 79646.6468
[2022-08-23 22:35:15,871 INFO] (isnngp_inference:99) Accuracy: 98.10%
[2022-08-23 22:35:16,179 INFO] (isnngp_inference:101) Loss: 0.1711
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 22:35:36,946 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=17000, select_method='random')
[2022-08-23 22:35:39,194 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 22:35:45,489 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 22:35:47,933 INFO] (snngp_inference:59) inducing_points shape: (17000, 784)
[2022-08-23 22:49:44,070 INFO] (isnngp_inference:93) ELBO: 40240.7755
[2022-08-23 22:58:17,869 INFO] (isnngp_inference:95) EUBO: 79586.1417
[2022-08-23 22:59:29,370 INFO] (isnngp_inference:99) Accuracy: 98.09%
[2022-08-23 22:59:29,654 INFO] (isnngp_inference:101) Loss: 0.1709
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 22:59:46,302 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=18000, select_method='random')
[2022-08-23 22:59:47,696 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 22:59:48,480 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 22:59:50,024 INFO] (snngp_inference:59) inducing_points shape: (18000, 784)
[2022-08-23 23:15:35,304 INFO] (isnngp_inference:93) ELBO: 40555.6820
[2022-08-23 23:24:05,368 INFO] (isnngp_inference:95) EUBO: 79528.6309
[2022-08-23 23:25:35,030 INFO] (isnngp_inference:99) Accuracy: 98.14%
[2022-08-23 23:25:35,334 INFO] (isnngp_inference:101) Loss: 0.1707
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 23:25:52,059 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=19000, select_method='random')
[2022-08-23 23:25:54,879 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 23:25:56,188 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 23:25:58,360 INFO] (snngp_inference:59) inducing_points shape: (19000, 784)
[2022-08-23 23:41:38,455 INFO] (isnngp_inference:93) ELBO: 40746.5112
[2022-08-23 23:51:08,630 INFO] (isnngp_inference:95) EUBO: 79480.4699
[2022-08-23 23:52:48,701 INFO] (isnngp_inference:99) Accuracy: 98.08%
[2022-08-23 23:52:48,962 INFO] (isnngp_inference:101) Loss: 0.1707
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-23 23:53:04,408 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist120k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist120k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=20000, select_method='random')
[2022-08-23 23:53:05,824 INFO] (data_loader:126) MNIST: (60000, 784) train, (10000, 784) test samples.
[2022-08-23 23:53:06,517 INFO] (data_loader:193) MNIST: (120000, 784) augmented train
[2022-08-23 23:53:08,178 INFO] (snngp_inference:59) inducing_points shape: (20000, 784)
[2022-08-24 00:10:24,435 INFO] (isnngp_inference:93) ELBO: 40989.8233
[2022-08-24 00:21:26,320 INFO] (isnngp_inference:95) EUBO: 79426.1642
[2022-08-24 00:23:15,145 INFO] (isnngp_inference:99) Accuracy: 98.13%
[2022-08-24 00:23:15,389 INFO] (isnngp_inference:101) Loss: 0.1706
