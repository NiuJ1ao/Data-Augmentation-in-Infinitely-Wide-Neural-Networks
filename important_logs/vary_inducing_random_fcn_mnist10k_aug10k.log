nohup: ignoring input
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 19:21:11,353 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=1000, select_method='random')
[2022-08-12 19:21:12,054 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 19:21:12,151 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 19:21:12,243 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 19:21:12,354 INFO] (snngp_inference:58) inducing_points shape: (1000, 784)
[2022-08-12 19:21:12,366 INFO] (snngp:272) Optimizing...
2022-08-12 19:21:19.174056: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 19:21:25.363248: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 7.18926477s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 19:21:27.897984: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  dot.5 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 19:23:48.003470: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2m22.105552224s
Constant folding an instruction is taking > 2s:

  dot.5 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-12 19:25:15,386 INFO] (snngp:280) Optimized for 15 iters; Success: True; Result: [1.09449507 0.1266518 ], 0.023871078745639426
[2022-08-12 19:26:11,748 INFO] (snngp_inference:96) LML: 10012.0979
[2022-08-12 19:26:16,760 INFO] (snngp_inference:98) ELBO: 7036.5089
[2022-08-12 19:26:17,546 INFO] (snngp_inference:100) EUBO: 18425.8397
[2022-08-12 19:26:20,653 INFO] (snngp_inference:104) Accuracy: 94.58%
[2022-08-12 19:26:20,754 INFO] (snngp_inference:106) Loss: 0.1354
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 19:26:37,167 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=2000, select_method='random')
[2022-08-12 19:26:37,906 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 19:26:38,007 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 19:26:38,103 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 19:26:38,239 INFO] (snngp_inference:58) inducing_points shape: (2000, 784)
[2022-08-12 19:26:38,257 INFO] (snngp:272) Optimizing...
2022-08-12 19:26:45.439322: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 19:27:13.007692: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 28.568449646s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 19:27:15.568102: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  dot.5 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 19:32:00.167883: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 4m46.599850424s
Constant folding an instruction is taking > 2s:

  dot.5 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-12 19:35:31,128 INFO] (snngp:280) Optimized for 12 iters; Success: True; Result: [1.13731495 0.15018595], 0.019734296244897576
[2022-08-12 19:36:27,237 INFO] (snngp_inference:96) LML: 11032.3298
[2022-08-12 19:36:35,634 INFO] (snngp_inference:98) ELBO: 8476.9146
[2022-08-12 19:36:38,126 INFO] (snngp_inference:100) EUBO: 19998.3568
[2022-08-12 19:36:42,717 INFO] (snngp_inference:104) Accuracy: 95.58%
[2022-08-12 19:36:42,814 INFO] (snngp_inference:106) Loss: 0.1248
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 19:36:59,126 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=3000, select_method='random')
[2022-08-12 19:36:59,880 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 19:36:59,981 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 19:37:00,073 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 19:37:00,200 INFO] (snngp_inference:58) inducing_points shape: (3000, 784)
[2022-08-12 19:37:00,215 INFO] (snngp:272) Optimizing...
2022-08-12 19:37:07.673136: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 19:38:11.114701: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1m4.441639432s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-12 19:45:42,726 INFO] (snngp:280) Optimized for 13 iters; Success: True; Result: [1.16070079 0.15415323], 0.01777113339511453
[2022-08-12 19:46:40,470 INFO] (snngp_inference:96) LML: 11565.3073
[2022-08-12 19:46:53,623 INFO] (snngp_inference:98) ELBO: 9229.2953
[2022-08-12 19:46:58,642 INFO] (snngp_inference:100) EUBO: 20778.4384
[2022-08-12 19:47:04,881 INFO] (snngp_inference:104) Accuracy: 95.99%
[2022-08-12 19:47:05,017 INFO] (snngp_inference:106) Loss: 0.1201
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 19:47:21,266 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=4000, select_method='random')
[2022-08-12 19:47:21,952 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 19:47:22,041 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 19:47:22,125 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 19:47:22,230 INFO] (snngp_inference:58) inducing_points shape: (4000, 784)
[2022-08-12 19:47:22,241 INFO] (snngp:272) Optimizing...
2022-08-12 19:47:29.584223: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 19:49:23.174639: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1m54.590492273s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-12 20:02:01,353 INFO] (snngp:280) Optimized for 13 iters; Success: True; Result: [1.18038175 0.15653206], 0.016298703082530477
[2022-08-12 20:02:58,564 INFO] (snngp_inference:96) LML: 11988.8165
[2022-08-12 20:03:18,718 INFO] (snngp_inference:98) ELBO: 9799.4806
[2022-08-12 20:03:27,667 INFO] (snngp_inference:100) EUBO: 21382.1664
[2022-08-12 20:03:35,513 INFO] (snngp_inference:104) Accuracy: 96.21%
[2022-08-12 20:03:35,614 INFO] (snngp_inference:106) Loss: 0.1169
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 20:03:51,877 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=5000, select_method='random')
[2022-08-12 20:03:52,542 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 20:03:52,641 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 20:03:52,736 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 20:03:52,879 INFO] (snngp_inference:58) inducing_points shape: (5000, 784)
[2022-08-12 20:03:52,897 INFO] (snngp:272) Optimizing...
2022-08-12 20:04:00.479860: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 20:06:58.269032: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2m58.789245982s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-12 20:26:24,407 INFO] (snngp:280) Optimized for 13 iters; Success: True; Result: [1.19590285 0.15711626], 0.01522851108809339
[2022-08-12 20:27:19,665 INFO] (snngp_inference:96) LML: 12310.5480
[2022-08-12 20:27:47,119 INFO] (snngp_inference:98) ELBO: 10222.6824
[2022-08-12 20:28:00,737 INFO] (snngp_inference:100) EUBO: 21813.1076
[2022-08-12 20:28:10,995 INFO] (snngp_inference:104) Accuracy: 96.39%
[2022-08-12 20:28:11,094 INFO] (snngp_inference:106) Loss: 0.1146
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 20:28:27,460 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=6000, select_method='random')
[2022-08-12 20:28:28,142 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 20:28:28,229 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 20:28:28,312 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 20:28:28,423 INFO] (snngp_inference:58) inducing_points shape: (6000, 784)
[2022-08-12 20:28:28,435 INFO] (snngp:272) Optimizing...
2022-08-12 20:28:36.009280: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 20:33:05.509812: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 4m30.500613267s
Constant folding an instruction is taking > 1s:

  dot.2 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 20:33:09.834705: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 2s:

  multiply.445 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-12 20:33:10.242362: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2.40773834s
Constant folding an instruction is taking > 2s:

  multiply.445 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-12 21:01:09,395 INFO] (snngp:280) Optimized for 13 iters; Success: True; Result: [1.21009411 0.15525102], 0.014273921233203191
[2022-08-12 21:02:05,506 INFO] (snngp_inference:96) LML: 12605.4485
[2022-08-12 21:02:42,670 INFO] (snngp_inference:98) ELBO: 10609.5237
[2022-08-12 21:03:02,523 INFO] (snngp_inference:100) EUBO: 22195.2802
[2022-08-12 21:03:15,166 INFO] (snngp_inference:104) Accuracy: 96.69%
[2022-08-12 21:03:15,263 INFO] (snngp_inference:106) Loss: 0.1127
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 21:03:31,917 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=7000, select_method='random')
[2022-08-12 21:03:32,663 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 21:03:32,759 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 21:03:32,848 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 21:03:32,970 INFO] (snngp_inference:58) inducing_points shape: (7000, 784)
[2022-08-12 21:03:32,982 INFO] (snngp:272) Optimizing...
[2022-08-12 21:42:06,418 INFO] (snngp:280) Optimized for 13 iters; Success: True; Result: [1.22398567 0.15624214], 0.013460981085508304
[2022-08-12 21:43:02,917 INFO] (snngp_inference:96) LML: 12867.3047
[2022-08-12 21:43:51,181 INFO] (snngp_inference:98) ELBO: 10932.8365
[2022-08-12 21:44:17,955 INFO] (snngp_inference:100) EUBO: 22503.8157
[2022-08-12 21:44:33,658 INFO] (snngp_inference:104) Accuracy: 96.77%
[2022-08-12 21:44:33,758 INFO] (snngp_inference:106) Loss: 0.1111
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 21:44:50,088 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=8000, select_method='random')
[2022-08-12 21:44:50,795 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 21:44:50,897 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 21:44:50,990 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 21:44:51,111 INFO] (snngp_inference:58) inducing_points shape: (8000, 784)
[2022-08-12 21:44:51,123 INFO] (snngp:272) Optimizing...
[2022-08-12 22:35:55,230 INFO] (snngp:280) Optimized for 13 iters; Success: True; Result: [1.23523604 0.15648302], 0.012766789066742194
[2022-08-12 22:36:51,174 INFO] (snngp_inference:96) LML: 13088.6385
[2022-08-12 22:37:53,060 INFO] (snngp_inference:98) ELBO: 11220.1483
[2022-08-12 22:38:27,713 INFO] (snngp_inference:100) EUBO: 22771.4232
[2022-08-12 22:38:46,766 INFO] (snngp_inference:104) Accuracy: 96.91%
[2022-08-12 22:38:46,899 INFO] (snngp_inference:106) Loss: 0.1099
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 22:39:03,065 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=9000, select_method='random')
[2022-08-12 22:39:03,777 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 22:39:03,875 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 22:39:03,969 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 22:39:04,099 INFO] (snngp_inference:58) inducing_points shape: (9000, 784)
[2022-08-12 22:39:04,114 INFO] (snngp:272) Optimizing...
[2022-08-12 23:40:22,200 INFO] (snngp:280) Optimized for 12 iters; Success: True; Result: [1.24727534 0.15386349], 0.012012294064709458
[2022-08-12 23:41:18,210 INFO] (snngp_inference:96) LML: 13332.8612
[2022-08-12 23:42:34,569 INFO] (snngp_inference:98) ELBO: 11531.0369
[2022-08-12 23:43:18,616 INFO] (snngp_inference:100) EUBO: 23070.7215
[2022-08-12 23:43:40,868 INFO] (snngp_inference:104) Accuracy: 97.16%
[2022-08-12 23:43:40,966 INFO] (snngp_inference:106) Loss: 0.1084
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-12 23:43:57,093 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=10000, select_method='random')
[2022-08-12 23:43:57,799 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-12 23:43:57,893 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-12 23:43:57,984 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-12 23:43:58,029 INFO] (snngp_inference:58) inducing_points shape: (10000, 784)
[2022-08-12 23:43:58,042 INFO] (snngp:272) Optimizing...
[2022-08-13 01:38:44,801 INFO] (snngp:280) Optimized for 12 iters; Success: True; Result: [1.25781166 0.15355138], 0.011418716924198897
[2022-08-13 01:40:12,693 INFO] (snngp_inference:96) LML: 13530.6641
[2022-08-13 01:42:32,819 INFO] (snngp_inference:98) ELBO: 11777.2448
[2022-08-13 01:44:00,589 INFO] (snngp_inference:100) EUBO: 23281.0968
[2022-08-13 01:44:38,955 INFO] (snngp_inference:104) Accuracy: 97.13%
[2022-08-13 01:44:39,092 INFO] (snngp_inference:106) Loss: 0.1074
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-13 01:44:55,433 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=11000, select_method='random')
[2022-08-13 01:44:56,419 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-13 01:44:56,548 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-13 01:44:56,685 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-13 01:44:56,877 INFO] (snngp_inference:58) inducing_points shape: (11000, 784)
[2022-08-13 01:44:56,898 INFO] (snngp:272) Optimizing...
2022-08-13 01:45:10.378684: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-13 01:45:10.457099: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1.078506366s
Constant folding an instruction is taking > 1s:

  dot.12 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-13 04:13:17,737 INFO] (snngp:280) Optimized for 12 iters; Success: True; Result: [1.270874   0.15228859], 0.010672121303876547
[2022-08-13 04:14:40,599 INFO] (snngp_inference:96) LML: 13780.5152
[2022-08-13 04:17:36,137 INFO] (snngp_inference:98) ELBO: 12088.4656
[2022-08-13 04:19:22,764 INFO] (snngp_inference:100) EUBO: 23569.1362
[2022-08-13 04:20:10,692 INFO] (snngp_inference:104) Accuracy: 97.19%
[2022-08-13 04:20:10,795 INFO] (snngp_inference:106) Loss: 0.1062
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-13 04:20:27,122 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=12000, select_method='random')
[2022-08-13 04:20:28,062 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-13 04:20:28,208 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-13 04:20:28,350 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-13 04:20:28,547 INFO] (snngp_inference:58) inducing_points shape: (12000, 784)
[2022-08-13 04:20:28,564 INFO] (snngp:272) Optimizing...
2022-08-13 04:20:41.188821: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.3 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-13 04:20:41.226330: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1.037593417s
Constant folding an instruction is taking > 1s:

  dot.3 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-13 07:10:07,365 INFO] (snngp:280) Optimized for 12 iters; Success: True; Result: [1.28232193 0.15119465], 0.01004298822775513
[2022-08-13 07:11:17,815 INFO] (snngp_inference:96) LML: 13994.8812
[2022-08-13 07:14:05,951 INFO] (snngp_inference:98) ELBO: 12354.6186
[2022-08-13 07:15:49,966 INFO] (snngp_inference:100) EUBO: 23791.0720
[2022-08-13 07:16:34,595 INFO] (snngp_inference:104) Accuracy: 97.27%
[2022-08-13 07:16:34,750 INFO] (snngp_inference:106) Loss: 0.1051
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-13 07:16:51,302 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=13000, select_method='random')
[2022-08-13 07:16:52,276 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-13 07:16:52,428 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-13 07:16:52,586 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-13 07:16:52,770 INFO] (snngp_inference:58) inducing_points shape: (13000, 784)
[2022-08-13 07:16:52,787 INFO] (snngp:272) Optimizing...
2022-08-13 07:17:04.442977: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.3 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-13 07:17:04.507356: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1.064462108s
Constant folding an instruction is taking > 1s:

  dot.3 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-13 09:32:42,551 INFO] (snngp:280) Optimized for 12 iters; Success: True; Result: [1.29493219 0.15090514], 0.009401275702053761
[2022-08-13 09:33:29,949 INFO] (snngp_inference:96) LML: 14219.0488
[2022-08-13 09:35:42,557 INFO] (snngp_inference:98) ELBO: 12628.8782
[2022-08-13 09:37:03,475 INFO] (snngp_inference:100) EUBO: 23992.0668
[2022-08-13 09:37:39,831 INFO] (snngp_inference:104) Accuracy: 97.34%
[2022-08-13 09:37:39,929 INFO] (snngp_inference:106) Loss: 0.1041
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-13 09:37:57,626 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=14000, select_method='random')
[2022-08-13 09:37:58,329 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-13 09:37:58,430 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-13 09:37:58,522 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-13 09:37:58,651 INFO] (snngp_inference:58) inducing_points shape: (14000, 784)
[2022-08-13 09:37:58,663 INFO] (snngp:272) Optimizing...
[2022-08-13 12:03:56,339 INFO] (snngp:280) Optimized for 11 iters; Success: True; Result: [1.30764081 0.14918212], 0.00867869507729073
[2022-08-13 12:04:44,403 INFO] (snngp_inference:96) LML: 14466.4937
[2022-08-13 12:07:17,757 INFO] (snngp_inference:98) ELBO: 12945.9943
[2022-08-13 12:08:52,200 INFO] (snngp_inference:100) EUBO: 24273.8962
[2022-08-13 12:09:33,818 INFO] (snngp_inference:104) Accuracy: 97.41%
[2022-08-13 12:09:33,917 INFO] (snngp_inference:106) Loss: 0.1030
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-13 12:09:51,625 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=15000, select_method='random')
[2022-08-13 12:09:52,319 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-13 12:09:52,417 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-13 12:09:52,507 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-13 12:09:52,639 INFO] (snngp_inference:58) inducing_points shape: (15000, 784)
[2022-08-13 12:09:52,654 INFO] (snngp:272) Optimizing...
[2022-08-13 15:57:16,741 INFO] (snngp:280) Optimized for 13 iters; Success: True; Result: [1.32212117 0.14700327], 0.007907537161277874
[2022-08-13 15:58:21,307 INFO] (snngp_inference:96) LML: 14738.2083
[2022-08-13 16:02:13,561 INFO] (snngp_inference:98) ELBO: 13283.8370
[2022-08-13 16:04:37,740 INFO] (snngp_inference:100) EUBO: 24555.2903
[2022-08-13 16:05:34,767 INFO] (snngp_inference:104) Accuracy: 97.43%
[2022-08-13 16:05:34,871 INFO] (snngp_inference:106) Loss: 0.1020
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-13 16:05:50,932 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=16000, select_method='random')
[2022-08-13 16:05:51,632 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-13 16:05:51,727 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-13 16:05:51,821 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-13 16:05:51,980 INFO] (snngp_inference:58) inducing_points shape: (16000, 784)
[2022-08-13 16:05:51,994 INFO] (snngp:272) Optimizing...
[2022-08-13 20:22:50,268 INFO] (snngp:280) Optimized for 12 iters; Success: True; Result: [1.33846592 0.14598403], 0.0070807670078511685
[2022-08-13 20:23:58,908 INFO] (snngp_inference:96) LML: 15035.6749
[2022-08-13 20:28:25,018 INFO] (snngp_inference:98) ELBO: 13661.2676
[2022-08-13 20:31:18,486 INFO] (snngp_inference:100) EUBO: 24834.3724
[2022-08-13 20:32:23,577 INFO] (snngp_inference:104) Accuracy: 97.49%
[2022-08-13 20:32:23,737 INFO] (snngp_inference:106) Loss: 0.1010
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-13 20:33:03,483 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=17000, select_method='random')
[2022-08-13 20:33:04,316 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-13 20:33:04,469 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-13 20:33:04,558 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-13 20:33:04,705 INFO] (snngp_inference:58) inducing_points shape: (17000, 784)
[2022-08-13 20:33:04,721 INFO] (snngp:272) Optimizing...
[2022-08-14 01:46:55,641 INFO] (snngp:280) Optimized for 13 iters; Success: True; Result: [1.35776833 0.14410652], 0.0060688185007728565
[2022-08-14 01:48:01,078 INFO] (snngp_inference:96) LML: 15403.2711
[2022-08-14 01:53:01,049 INFO] (snngp_inference:98) ELBO: 14140.3320
[2022-08-14 01:56:07,713 INFO] (snngp_inference:100) EUBO: 25231.0225
[2022-08-14 01:57:19,514 INFO] (snngp_inference:104) Accuracy: 97.56%
[2022-08-14 01:57:19,629 INFO] (snngp_inference:106) Loss: 0.0997
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-14 11:45:48,549 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=18000, select_method='random')
[2022-08-14 11:45:50,176 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-14 11:45:50,599 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-14 11:45:50,763 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-14 11:45:51,036 INFO] (snngp_inference:58) inducing_points shape: (18000, 784)
[2022-08-14 11:45:51,066 INFO] (snngp:272) Optimizing...
2022-08-14 11:46:10.836231: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.0 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-14 11:46:11.247855: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1.411777597s
Constant folding an instruction is taking > 1s:

  dot.0 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-14 16:00:25,969 INFO] (snngp:280) Optimized for 12 iters; Success: True; Result: [1.3791149  0.14162027], 0.005070797119995916
[2022-08-14 16:01:48,417 INFO] (snngp_inference:96) LML: 15784.3683
[2022-08-14 16:05:53,755 INFO] (snngp_inference:98) ELBO: 14620.8570
[2022-08-14 16:07:33,938 INFO] (snngp_inference:100) EUBO: 25540.0552
[2022-08-14 16:08:44,751 INFO] (snngp_inference:104) Accuracy: 97.63%
[2022-08-14 16:08:45,013 INFO] (snngp_inference:106) Loss: 0.0985
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-14 16:09:09,437 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=19000, select_method='random')
[2022-08-14 16:09:10,772 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-14 16:09:10,962 INFO] (data_loader:157) MNIST: (10000, 784) augmented train
[2022-08-14 16:09:11,092 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-14 16:09:11,358 INFO] (snngp_inference:58) inducing_points shape: (19000, 784)
[2022-08-14 16:09:11,387 INFO] (snngp:272) Optimizing...
2022-08-14 16:09:30.374680: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.0 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-14 16:09:30.604949: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1.230425852s
Constant folding an instruction is taking > 1s:

  dot.0 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-14 20:55:35,217 INFO] (snngp:280) Optimized for 14 iters; Success: True; Result: [1.41081094 0.13923188], 0.00354649783923321
[2022-08-14 20:56:19,475 INFO] (snngp_inference:96) LML: 16387.4914
[2022-08-14 20:59:45,102 INFO] (snngp_inference:98) ELBO: 15429.9326
[2022-08-14 21:02:09,618 INFO] (snngp_inference:100) EUBO: 26117.9143
[2022-08-14 21:03:17,733 INFO] (snngp_inference:104) Accuracy: 97.70%
[2022-08-14 21:03:17,955 INFO] (snngp_inference:106) Loss: 0.0970
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
[2022-08-14 21:03:29,682 INFO] (util:27) Namespace(augment_X='/vol/bitbucket/yn621/data/mnist10k-augs-patterns.npy', augment_y='/vol/bitbucket/yn621/data/mnist10k-augs-labels.npy', batch_size=0, device_count=-1, epochs=1, lr=0.1, model='fcn', momentum=0.9, num_inducing_points=20000, select_method='random')
[2022-08-14 21:03:30,450 INFO] (data_loader:92) MNIST: (10000, 784) train, (10000, 784) test samples.
[2022-08-14 21:03:30,606 INFO] (data_loader:191) MNIST: (10000, 784) augmented train
[2022-08-14 21:03:30,727 INFO] (snngp_inference:46) Training data after augmentation: (20000, 784), (20000, 10)
[2022-08-14 21:03:30,939 INFO] (snngp_inference:58) inducing_points shape: (20000, 784)
[2022-08-14 21:03:30,961 INFO] (snngp:272) Optimizing...
2022-08-14 21:03:48.395074: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:

  dot.0 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2022-08-14 21:03:48.611575: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 1.216657154s
Constant folding an instruction is taking > 1s:

  dot.0 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above).

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
[2022-08-15 00:23:55,688 INFO] (snngp:280) Optimized for 17 iters; Success: True; Result: [1.49151748 0.13615281], 0.00011296992107433153
[2022-08-15 00:24:26,817 INFO] (snngp_inference:96) LML: 18149.1332
[2022-08-15 00:26:54,327 INFO] (snngp_inference:98) ELBO: 18061.4157
[2022-08-15 00:28:12,031 INFO] (snngp_inference:100) EUBO: 24149.0333
[2022-08-15 00:29:03,755 INFO] (snngp_inference:104) Accuracy: 97.84%
[2022-08-15 00:29:03,917 INFO] (snngp_inference:106) Loss: 0.0943
